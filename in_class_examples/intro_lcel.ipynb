{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic and common use case is chaining a prompt template and a model together. To see how this works, let's create a chain that takes a topic and generates a joke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the velvet cloak of night,  \\nThe moon hangs high, a silver light,  \\nA lantern in the vast expanse,  \\nWhispering secrets in a dance.  \\n\\nHer glow is soft, a tender balm,  \\nA soothing spell, a tranquil calm,  \\nShe bathes the world in gentle beams,  \\nAwakening the night with dreams.  \\n\\nThe stars, they twinkle, bow in grace,  \\nWhile shadows play in her embrace,  \\nThe trees stand tall, their whispers low,  \\nAs moonlit rivers weave and flow.  \\n\\nShe pulls the tides, a silent guide,  \\nA guardian of the ocean's stride,  \\nIn every phase, from crescent new,  \\nTo full and bright, her magic's true.  \\n\\nOh, moon of silver, calm and bright,  \\nYou cradle hopes in your soft light,  \\nA muse for lovers, a friend to night,  \\nIn your embrace, the world feels right.  \\n\\nSo here we stand, beneath your glow,  \\nIn awe of all the love you show,  \\nForever in your gentle sway,  \\nThe moon shall light the night away.  \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a poem about {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"the moon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this line of the code, where we piece together these different components into a single chain using LCEL:\n",
    "\n",
    "`chain = prompt | model | output_parser`\n",
    "\n",
    "The | symbol is similar to a unix pipe operator, which chains together the different components, feeding the output from one component as input into the next component.\n",
    "\n",
    "In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let's take a look at each component individually to really understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a poem about the moon', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"the moon\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a poem about the moon', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a poem about the moon'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "`PromptValue` is passed sequentially to `model`. This will output a `BaseMessage`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In the velvet cloak of night,  \\nThe moon ascends, a silver light,  \\nA watchful eye in skies so deep,  \\nWhere dreams are born and secrets keep.  \\n\\nShe bathes the world in gentle glow,  \\nKissing the rivers, soft and slow,  \\nWhispers of stories, old and wise,  \\nReflect in her calm, ethereal rise.  \\n\\nWith craters etched like tales long told,  \\nOf cosmic dances, brave and bold,  \\nShe pulls the tides with tender grace,  \\nA silent guardian of time and space.  \\n\\nIn her embrace, the shadows play,  \\nWhile starlit whispers drift away,  \\nA tapestry of night unfurls,  \\nAs she weaves magic through the swirls.  \\n\\nOh, luminous orb of distant skies,  \\nYou hold our hopes, our starlit sighs,  \\nWith every phase, a chance to dream,  \\nIn your soft glow, we find our gleam.  \\n\\nSo here beneath your watchful gaze,  \\nWe pause, we ponder, we explore the maze,  \\nFor in your light, we find our tune,  \\nForever enchanted by the moon.  ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 235, 'prompt_tokens': 14, 'total_tokens': 249, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-5bd57466-a4cc-4bc7-bc0f-14e02b38afe0-0', usage_metadata={'input_tokens': 14, 'output_tokens': 235, 'total_tokens': 249})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nIn the dark of night, she shines so bright\\nA silvery orb, a mystical sight\\nThe moon, a celestial wonder above\\nGuiding sailors, and inspiring love\\n\\nShe waxes and wanes, a constant dance\\nA symbol of change, of second chance\\nShe pulls the tides, and rules the sea\\nBut her true power, is in her mystery\\n\\nA lonely traveler, in the vast expanse\\nHer cratered surface, a poetic trance\\nShe watches over us, with a tranquil grace\\nA calming presence, in this chaotic space\\n\\nThe moon, a muse for poets and dreamers\\nHer gentle glow, a source of gleamers\\nA beacon of hope, in the darkest of night\\nA reminder of beauty, in every sight\\n\\nSo let us gaze, at her glowing face\\nAnd lose ourselves, in her quiet embrace\\nFor the moon, in all her celestial glory\\nIs a timeless symbol, of nature's story.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A model as an LLM will output a string\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parser\n",
    "And lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The specific StrOutputParser simply converts any input into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the velvet cloak of night,  \\nThe moon ascends, a silver light,  \\nA watchful eye in skies so deep,  \\nWhere dreams are born and secrets keep.  \\n\\nShe bathes the world in gentle glow,  \\nKissing the rivers, soft and slow,  \\nWhispers of stories, old and wise,  \\nReflect in her calm, ethereal rise.  \\n\\nWith craters etched like tales long told,  \\nOf cosmic dances, brave and bold,  \\nShe pulls the tides with tender grace,  \\nA silent guardian of time and space.  \\n\\nIn her embrace, the shadows play,  \\nWhile starlit whispers drift away,  \\nA tapestry of night unfurls,  \\nAs she weaves magic through the swirls.  \\n\\nOh, luminous orb of distant skies,  \\nYou hold our hopes, our starlit sighs,  \\nWith every phase, a chance to dream,  \\nIn your soft glow, we find our gleam.  \\n\\nSo here beneath your watchful gaze,  \\nWe pause, we ponder, we explore the maze,  \\nFor in your light, we find our tune,  \\nForever enchanted by the moon.  '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipeline\n",
    "To follow the steps along:\n",
    "\n",
    "- We pass in user input on the desired topic as `{\"topic\": \"ice cream\"}`\n",
    "- The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt.\n",
    "- The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a `ChatMessage` object.\n",
    "- Finally, the `output_parser` component takes in a `ChatMessage`, and transforms this into a Python string, which is returned from the invoke method.\n",
    "\n",
    "<img src=\"../images/chain_image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='tell me a poem about the moon', additional_kwargs={}, response_metadata={})]\n",
      "content='In the velvet sky, a lantern glows,  \\nWhispers of silver where the night wind blows.  \\nA guardian of dreams, in tranquil embrace,  \\nThe moon weaves her magic, a soft, tender lace.  \\n\\nShe dances on waters with a shimmering grace,  \\nPainting the shadows, a celestial trace.  \\nHer beams like a lullaby, gentle and sweet,  \\nGuiding the lost on their winding retreat.  \\n\\nIn the hush of the night, when the world holds its breath,  \\nShe cradles the secrets, the stories of death.  \\nFrom lovers’ soft sighs to the cries of the trees,  \\nThe moon listens closely, a witness to pleas.  \\n\\nOh, how she changes, a fickle old friend,  \\nFrom crescent to full, on her cycles depend.  \\nA beacon of solace in darkness profound,  \\nIn her quiet presence, true peace can be found.  \\n\\nSo here’s to the moon, in her luminous flight,  \\nA symbol of hope in the depths of the night.  \\nMay we gaze upon her, with wonder anew,  \\nFor in her pale glow, all our dreams can come true.  ' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 237, 'prompt_tokens': 14, 'total_tokens': 251, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-d3459e5c-a636-40d0-a852-ac4a73fa9ed0-0' usage_metadata={'input_tokens': 14, 'output_tokens': 237, 'total_tokens': 251}\n"
     ]
    }
   ],
   "source": [
    "input = {\"topic\": \"the moon\"}\n",
    "\n",
    "print(prompt.invoke(input))\n",
    "\n",
    "\n",
    "print((prompt | model).invoke(input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauntletpaul/class-5-chaining-with-lcel/.venv/lib/python3.12/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context.\n",
    "\n",
    "As a preliminary step, we’ve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component that can be chained together with other components, but you can also try to run it separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='harrison worked at kensho'),\n",
       " Document(metadata={}, page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow being:\n",
    "\n",
    "- The first steps create a RunnableParallel object with two entries. The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the user’s original question. To pass on the question, we use RunnablePassthrough to copy this entry.\n",
    "- Feed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue.\n",
    "- The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\n",
    "- Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\n",
    "\n",
    "<img src=\"../images/chain_parallel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down? \\n\\nBecause it couldn’t find its “sundae” driver!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return call_chat_model(messages)\n",
    "\n",
    "invoke_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down?  \\n\\nBecause it had a rocky road!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream truck break down? \n",
      "\n",
      "Because it had too many scoop-ache!"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def stream_chat_model(messages: List[dict]) -> Iterator[str]:\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    for response in stream:\n",
    "        content = response.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            yield content\n",
    "\n",
    "def stream_chain(topic: str) -> Iterator[str]:\n",
    "    prompt_value = prompt.format(topic=topic)\n",
    "    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n",
    "\n",
    "\n",
    "for chunk in stream_chain(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream truck break down? \n",
      "\n",
      "Because it had a rocky road!"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What did the ice cream cone say to the refrigerator? \\n\\n“I’ve got to chill out!”',\n",
       " 'Why did the spaghetti break up with the meatball? \\n\\nBecause it couldn’t handle the sauce of the relationship!',\n",
       " \"Why did the dumpling go to therapy?  \\n\\nBecause it couldn't stop feeling so stuffed!\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def batch_chain(topics: list) -> list:\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(executor.map(invoke_chain, topics))\n",
    "\n",
    "batch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What did the ice cream cone say to the scoop? \\n\\n\"I scream, you scream, we all scream for ice cream!\" 🍦',\n",
       " 'Why did the spaghetti break up with the meatball? \\n\\nBecause it couldn’t handle the pressure!',\n",
       " 'Why did the dumpling break up with the noodle?  \\n\\nBecause it found someone who really knew how to wrap it up!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream cone become a chef?  \\n\\nBecause it wanted to make \"scoop\"-tacular dishes!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async_client = openai.AsyncOpenAI()\n",
    "\n",
    "async def acall_chat_model(messages: List[dict]) -> str:\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def ainvoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return await acall_chat_model(messages)\n",
    "\n",
    "\n",
    "await ainvoke_chain(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream cone break up with the sundae? \\n\\nBecause it found someone cooler! 🍦'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke(\"ice cream\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
